paste("story",c(1:10),sep="_")
paste("story",c(1:50),sep="_")
storylib <- paste("story",c(1:50),sep="_")
storiesTagged_A <- sample()
sample(2, storylib)
sample(storylib,2)
?sample
sample(storylib,2,replace=F)
sample(storylib,5,replace=F)
storiesTagged_A <- sample(storylib,5,replace=F)
storylib <- paste("story",c(1:20),sep="_")
storiesTagged_A <- sample(storylib,5,replace=F)
storiesTagged_B <- sample(storylib,3,replace=F)
storiesTagged_C <- sample(storylib,10,replace=F)
storiesTagged_A
storiesTagged_B
storiesTagged_C
match(storiesTagged_B, storiesTagged_C)
storylib
storiesTagged_A
install.packages("leafletR")
install.packages("rvest")
library(rvest)
topicpages <- html("http://www.bbc.co.uk/news/topics?list=true")
topicpages
topicpages %>%
html_node(".topic-url") %>%
html_text()
topicpages %>%
html_nodes(".topic-url") %>%
html_text()
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(by="/")
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit()
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/")
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
lapply(strsplit(split="/"), "[[", 1)
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 1)
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 2)
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/")
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 4)
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 4) %>%
unlist()
topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 4) %>%
unlist() %>%
m
tIDs <- topicpages %>%
html_nodes(".topic-url") %>%
html_text() %>%
strsplit(split="/") %>%
lapply("[[", 4) %>%
unlist()
tIDs
response <- http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE
response <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE"
response
libray(curl)
library(curl)
response <- getURL("http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE")
url <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE")
url <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE")
url <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE"
r <- GET(url)
library(httr)
r <- GET(url)
r
url
ldpresponse %>%
html_nodes(".urn:bbc:cps:asset:36357212l") %>%
html_text()
ldpresponse <- GET(url)
ldpresponse %>%
html_nodes(".urn:bbc:cps:asset:36357212l") %>%
html_text()
a <- htmlParse(ldpresponse)
require(RCurl)
require(XML)
a <- htmlParse(ldpresponse)
a
ldpresponse
a
ldpresponse
webpage <- getURL(url, header=FALSE, verbose=TRUE)
library(RCurl)
install.packages("RCurl")
require(XML)
url <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE"
webpage <- getURL(url, header=FALSE, verbose=TRUE)
webpage <- getURL(url, header=FALSE, verbose=TRUE)
get(url)
url <- "http://ldp-core.api.bbci.co.uk/ldp-core/creative-works-v2?about=ed9d1ef3-eded-4f81-b158-be49cfc1ea8f&api_key=CxT3LtmheFohKE0gTPRE9pXg2zS3qykE"
GET(url)
response <- GET(url)
ldpresponse <- GET(url)
hh <- htmlParse(ldpresponse, asText=T)
hh
ldpresponse %>%
html_nodes(".urn:bbc:cps:asset:*") %>%
html_text()
a <- c(20, 60, 100, 140, 180, 220, 260, 300, 340, 380, 420, 460, 500, 540, 580, 620, 660, 700, 740, 780, 820, 860, 900, 940, 980)
b<-c(0.7883, 0.5767, 0.4602, 0.3883, 0.3475, 0.2984, 0.2833, 0.2561, 0.2386, 0.2294, 0.213, 0.2057, 0.1982, 0.1884, 0.1865, 0.183, 0.175, 0.1719, 0.1713, 0.1614, 0.1663, 0.1574, 0.1593, 0.1513, 0.154)
plot(a,b)
url
a <- c(0.4992, 0.3658, 0.3163, 0.2527, 0.2353, 0.2116, 0.184, 0.1918, 0.1769, 0.1618, 0.149, 0.1388, 0.13, 0.1247, 0.122, 0.1152, 0.1106, 0.1098, 0.1058, 0.1036, 0.1024, 0.1083, 0.1052, 0.1042, 0.1115, 0.1054, 0.1033, 0.1095, 0.1058, 0.1219, 0.1179, 0.1123, 0.108, 0.1201, 0.1227, 0.1281, 0.1176, 0.133, 0.1363, 0.142, 0.1373, 0.1387, 0.1408, 0.1483, 0.1574, 0.1528, 0.1553)
seq(20,940, 20)
b<-seq(20,940, 20)
plot(b,a)
library(ggplot2)
ggplot()
data.frame(nrTopics=b, topicSimilarity=a)
data.frame(nrTopics=b, topicSimilarity=a)->df
ggplot(df, aes(nrTopics, topicSimilarity)) + geom_point()
# ------------------------------------------------------------------------
# Step 1: Get the data
# ------------------------------------------------------------------------
library(RCurl)
library(XML)
library(jsonlite)
library(plyr)
library(reshape)
library(ggplot2)
## -------- sources
# ID  Name
sourceIDs <- c(160, 22, 8)
sourcestring <- ""
for(s in sourceIDs){sourcestring <- paste(sourcestring,"&sources%5B%5D=",s, sep="")}
sourcestring
sourceIDs <- c(1)
sourcestring <- ""
for(s in sourceIDs){sourcestring <- paste(sourcestring,"&sources%5B%5D=",s, sep="")}
sourcestring
## -------- Dates
# before = "2015-06-01"
# after = "2015-05-01"
## -------- API key
key = 'ZtGNiDhnYDsoSRA1GiJDHfs3KtnlgGox'
## -------- simple API query
smallN = 10
search <- paste("http://data.test.bbc.co.uk/bbcrd-juicer/articles?apikey=",
key,
"&recent_first",
sourcestring,
#"&published_after=",after,"T00:00:00.000Z",
#"&published_before=",before,"T00:00:00.000Z",
"&size=",smallN,
sep="")
search
before = "2016-05-01"
after = "2016-01-01"
search <- paste("http://data.test.bbc.co.uk/bbcrd-juicer/articles?apikey=",
key,
"&recent_first",
sourcestring,
"&published_after=",after,"T00:00:00.000Z",
"&published_before=",before,"T00:00:00.000Z",
"&size=",smallN,
sep="")
search <- paste("http://juicer.api.bbci.co.uk/articles?apikey=",
key,
"&recent_first",
sourcestring,
"&published_after=",after,"T00:00:00.000Z",
smallN = 10
search <- paste("http://juicer.api.bbci.co.uk/articles?apikey=",
key,
"&recent_first",
sourcestring,
"&published_after=",after,"T00:00:00.000Z",
"&published_before=",before,"T00:00:00.000Z",
"&size=",smallN,
sep="")
contentJSON <- getURL(search)
setwd("~/CloudStation/WORK/WORK-CONTENT/1504_BBC/TopicModels/tm-BBC-categories/datasets")
setwd("~/CloudStation/WORK/WORK-CONTENT/1504_BBC/TopicModels/tm-BBC-categories/tests/topics-smalltest")
library(plyr)
library(ggplot2)
col <- c("#4c2b26", "#ff4400", "#d9896c", "#d9c7a3", "#d9ad00",
"#454d26", "#55f23d", "#00660e", "#00f2c2", "#00ccff",
"#006680", "#2d3959", "#0022ff", "#7989f2", "#868cb3",
"#240059", "#9b00a6", "#a6537f", "#e5005c", "#8c0013")
# read document-topic-matrix and select most likely topic assignment for each article
docTopic <- read.delim("out/docTopic.csv", sep=",", skip=1, header=F)
smry <- ddply(docTopic, .(V2), summarise, maxProb=max(V4, na.rm=TRUE))
docTopic2 <- merge(docTopic, smry, by.x="V4", by.y="maxProb", all.x=FALSE, all.y=FALSE)
# read topic-word-matrix
topicWords <- read.delim("out/wordTopic.string", sep="\t", header=F)
df <- merge(docTopic2, topicWords, by.x="V3", by.y="V1")
levels(df$V2) <- gsub(",", "\n",levels(df$V2))
df$topic <- paste("topic #",df$V3+1,"\n\n",df$V2, sep="")
# subset to have an even category distribution
samplesize = min(summary(df$V1))
dfsub <- df[1,]
for(l in levels(df$V1)){
print(l)
print(dim(df[df$V1==l,])[1])
part <- df[df$V1==l,]
sampl <- part[sample(seq(1,dim(part)[1]), samplesize),]
dfsub <- rbind.data.frame(dfsub, sampl)
}
#dfsub$topic <- factor(dfsub$topic, levels=levels(factor(dfsub$topic))[c(1,2,5:12,3,4)])
#dfsub$cat <- factor(dfsub$V1, levels=levels(as.factor(dfsub$V1))[c(3,4,6,9,10,1,2,5,7,8,11,12)] )
ggplot(dfsub, aes(topic, fill=as.factor(V1))) +
geom_bar(width=0.7) +
scale_fill_manual(values=col) +
theme_bw() +
theme(panel.border=element_rect(colour=NA),
legend.key = element_rect(colour = NA)) +
labs(x="", fill="",y="number of articles")
setwd("~/CloudStation/WORK/WORK-CONTENT/1504_BBC/TopicModels/_tm-BBC-categories/tests/topics-smalltest")
setwd("~/CloudStation/WORK/WORK-CONTENT/1504_BBC/TopicModels/Text_Analytics/tm-bbc-categories/tests/topics-smalltest")
library(plyr)
library(ggplot2)
col <- c("#4c2b26", "#ff4400", "#d9896c", "#d9c7a3", "#d9ad00",
"#454d26", "#55f23d", "#00660e", "#00f2c2", "#00ccff",
"#006680", "#2d3959", "#0022ff", "#7989f2", "#868cb3",
"#240059", "#9b00a6", "#a6537f", "#e5005c", "#8c0013")
# read document-topic-matrix and select most likely topic assignment for each article
docTopic <- read.delim("out/docTopic.csv", sep=",", skip=1, header=F)
smry <- ddply(docTopic, .(V2), summarise, maxProb=max(V4, na.rm=TRUE))
docTopic2 <- merge(docTopic, smry, by.x="V4", by.y="maxProb", all.x=FALSE, all.y=FALSE)
# read topic-word-matrix
topicWords <- read.delim("out/wordTopic.string", sep="\t", header=F)
df <- merge(docTopic2, topicWords, by.x="V3", by.y="V1")
levels(df$V2) <- gsub(",", "\n",levels(df$V2))
df$topic <- paste("topic #",df$V3+1,"\n\n",df$V2, sep="")
# subset to have an even category distribution
samplesize = min(summary(df$V1))
dfsub <- df[1,]
for(l in levels(df$V1)){
print(l)
print(dim(df[df$V1==l,])[1])
part <- df[df$V1==l,]
sampl <- part[sample(seq(1,dim(part)[1]), samplesize),]
dfsub <- rbind.data.frame(dfsub, sampl)
}
ggplot(dfsub, aes(topic, fill=as.factor(V1))) +
geom_bar(width=0.7) +
scale_fill_manual(values=col) +
theme_bw() +
theme(panel.border=element_rect(colour=NA),
legend.key = element_rect(colour = NA)) +
labs(x="", fill="",y="number of articles")
head(dfsub)
table(dfsub$topic, dfsub$V1)
cm <- table(dfsub$topic, dfsub$V3)
cm
summary(dfsub)
table(dfsub$V1, dfsub$V3)
cm <- table(dfsub$V1, dfsub$V3)
confusionMatrix(cm)
library(caret)
cm <- table(dfsub$V1, dfsub$V3)
confusionMatrix(cm)
print(table(dfsub$V1, dfsub$V3))
